{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiMwCEaE9P9M",
        "outputId": "25745a82-e0c7-41ae-a8c9-e89c7987998c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frequent Pairs:\n",
            "+----------------------------------------------------+-------+\n",
            "|Pair                                                |Support|\n",
            "+----------------------------------------------------+-------+\n",
            "|frozenset({'sausage', 'whole milk'})                |134    |\n",
            "|frozenset({'whole milk', 'yogurt'})                 |167    |\n",
            "|frozenset({'whole milk', 'semi-finished bread'})    |25     |\n",
            "|frozenset({'sausage', 'yogurt'})                    |86     |\n",
            "|frozenset({'yogurt', 'semi-finished bread'})        |13     |\n",
            "|frozenset({'pastry', 'whole milk'})                 |97     |\n",
            "|frozenset({'salty snack', 'pastry'})                |10     |\n",
            "|frozenset({'salty snack', 'whole milk'})            |29     |\n",
            "|frozenset({'sausage', 'hygiene articles'})          |13     |\n",
            "|frozenset({'soda', 'pickled vegetables'})           |12     |\n",
            "|frozenset({'whole milk', 'rolls/buns'})             |209    |\n",
            "|frozenset({'sausage', 'rolls/buns'})                |80     |\n",
            "|frozenset({'curd', 'frankfurter'})                  |20     |\n",
            "|frozenset({'soda', 'whole milk'})                   |174    |\n",
            "|frozenset({'white bread', 'beef'})                  |13     |\n",
            "|frozenset({'soda', 'frankfurter'})                  |46     |\n",
            "|frozenset({'frankfurter', 'whipped/sour cream'})    |22     |\n",
            "|frozenset({'soda', 'whipped/sour cream'})           |51     |\n",
            "|frozenset({'other vegetables', 'frozen vegetables'})|47     |\n",
            "|frozenset({'whole milk', 'butter'})                 |70     |\n",
            "+----------------------------------------------------+-------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Association Rules:\n",
            "+----------------------+----------+-------------------+-------+\n",
            "|Antecedent            |Consequent|Confidence         |Support|\n",
            "+----------------------+----------+-------------------+-------+\n",
            "|softener              |whole milk|0.2926829268292683 |12     |\n",
            "|house keeping products|whole milk|0.24444444444444444|11     |\n",
            "|finished products     |whole milk|0.203125           |13     |\n",
            "|brandy                |whole milk|0.34210526315789475|13     |\n",
            "+----------------------+----------+-------------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import collect_set\n",
        "from pyspark.ml.param import Param, Params\n",
        "from pyspark.ml import Estimator, Model\n",
        "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
        "from pyspark.rdd import RDD\n",
        "from typing import List, Tuple, Optional\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# PCYParams class for managing parameters\n",
        "class PCYParams(Params, DefaultParamsReadable, DefaultParamsWritable):\n",
        "    minSupport = Param(Params._dummy(), \"minSupport\", \"Support threshold for frequent items\")\n",
        "    minConfidence = Param(Params._dummy(), \"minConfidence\", \"Confidence threshold for association rules\")\n",
        "    numBuckets = Param(Params._dummy(), \"numBuckets\", \"Number of buckets for hashing\")\n",
        "    itemsCol = Param(Params._dummy(), \"itemsCol\", \"Name of items column\")\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PCYParams, self).__init__()\n",
        "        self._setDefault(minSupport=10, minConfidence=0.2, numBuckets=100, itemsCol=\"items\")\n",
        "\n",
        "    def setMinSupport(self, value: int):\n",
        "        return self._set(minSupport=value)\n",
        "\n",
        "    def getMinSupport(self) -> int:\n",
        "        return self.getOrDefault(self.minSupport)\n",
        "\n",
        "    def setMinConfidence(self, value: float):\n",
        "        return self._set(minConfidence=value)\n",
        "\n",
        "    def getMinConfidence(self) -> float:\n",
        "        return self.getOrDefault(self.minConfidence)\n",
        "\n",
        "    def setNumBuckets(self, value: int):\n",
        "        return self._set(numBuckets=value)\n",
        "\n",
        "    def getNumBuckets(self) -> int:\n",
        "        return self.getOrDefault(self.numBuckets)\n",
        "\n",
        "    def setItemsCol(self, value: str):\n",
        "        return self._set(itemsCol=value)\n",
        "\n",
        "    def getItemsCol(self) -> str:\n",
        "        return self.getOrDefault(self.itemsCol)\n",
        "\n",
        "# PCY (Estimator) class for training the model\n",
        "class PCY(Estimator, PCYParams, DefaultParamsReadable, DefaultParamsWritable):\n",
        "    def __init__(self, minSupport: Optional[int] = None,\n",
        "                 minConfidence: Optional[float] = None,\n",
        "                 numBuckets: Optional[int] = None,\n",
        "                 itemsCol: Optional[str] = None):\n",
        "        super(PCY, self).__init__()\n",
        "\n",
        "        # Set parameters\n",
        "        if minSupport is not None:\n",
        "            self.setMinSupport(minSupport)\n",
        "        if minConfidence is not None:\n",
        "            self.setMinConfidence(minConfidence)\n",
        "        if numBuckets is not None:\n",
        "            self.setNumBuckets(numBuckets)\n",
        "        if itemsCol is not None:\n",
        "            self.setItemsCol(itemsCol)\n",
        "\n",
        "    def _hash_pair(self, pair: frozenset, num_buckets: int) -> int:\n",
        "        pair_tuple = tuple(sorted(pair))\n",
        "        return hash(pair_tuple) % num_buckets\n",
        "\n",
        "    def _fit(self, dataset: DataFrame) -> \"PCYModel\":\n",
        "        s = self.getMinSupport()\n",
        "        c = self.getMinConfidence()\n",
        "        num_buckets = self.getNumBuckets()\n",
        "        items_col = self.getItemsCol()\n",
        "\n",
        "        # Prepare data: Get carts from dataset\n",
        "        # If items_col column exists then use it, otherwise create\n",
        "        if items_col in dataset.columns:\n",
        "            baskets_rdd: RDD[List[str]] = dataset.select(items_col).rdd.map(lambda x: x[0])\n",
        "        else:\n",
        "            # Assuming the data format is as in the original example\n",
        "            baskets_df = dataset.groupBy(\"Member_number\", \"Date\").agg(collect_set(\"itemDescription\").alias(items_col))\n",
        "            baskets_rdd: RDD[List[str]] = baskets_df.rdd.map(lambda x: x[items_col])\n",
        "\n",
        "        # Pass 1: Create and count buckets\n",
        "        pairs_rdd = baskets_rdd.flatMap(lambda items: [frozenset([items[i], items[j]])\n",
        "                                                       for i in range(len(items))\n",
        "                                                       for j in range(i + 1, len(items))])\n",
        "        buckets_rdd = pairs_rdd.map(lambda pair: (self._hash_pair(pair, num_buckets), 1))\n",
        "        bucket_counts = buckets_rdd.reduceByKey(lambda a, b: a + b)\n",
        "        frequent_buckets = bucket_counts.filter(lambda x: x[1] >= s).collectAsMap()\n",
        "\n",
        "        # Create a bitmap(set) and broadcast it\n",
        "        sc = SparkContext.getOrCreate()\n",
        "        frequent_buckets_set = set(frequent_buckets.keys())\n",
        "        broadcast_frequent_buckets = sc.broadcast(frequent_buckets_set)\n",
        "\n",
        "        # Pass 2: Count pairs in common buckets using broadcast variable\n",
        "        def count_pairs(items: List[str]) -> List[Tuple[frozenset, int]]:\n",
        "            pairs = [frozenset([items[i], items[j]])\n",
        "                     for i in range(len(items))\n",
        "                     for j in range(i + 1, len(items))]\n",
        "            return [(pair, 1) for pair in pairs if self._hash_pair(pair, num_buckets) in broadcast_frequent_buckets.value]\n",
        "\n",
        "        pair_counts = baskets_rdd.flatMap(count_pairs).reduceByKey(lambda a, b: a + b)\n",
        "        frequent_pairs = pair_counts.filter(lambda x: x[1] >= s).collectAsMap()\n",
        "\n",
        "        # Calculate individual item support and create association rules\n",
        "        item_counts = baskets_rdd.flatMap(lambda items: [(item, 1) for item in items])\\\n",
        "                                 .reduceByKey(lambda a, b: a + b)\\\n",
        "                                 .collectAsMap()\n",
        "\n",
        "        rules = []\n",
        "        for pair, pair_support in frequent_pairs.items():\n",
        "            item1, item2 = list(pair)\n",
        "            conf1 = pair_support / item_counts[item1] if item_counts[item1] > 0 else 0\n",
        "            conf2 = pair_support / item_counts[item2] if item_counts[item2] > 0 else 0\n",
        "            if conf1 >= c:\n",
        "                rules.append((item1, item2, conf1, pair_support))\n",
        "            if conf2 >= c:\n",
        "                rules.append((item2, item1, conf2, pair_support))\n",
        "\n",
        "        # Returns the trained model\n",
        "        return PCYModel(self.getMinSupport(), self.getMinConfidence(),\n",
        "                       self.getNumBuckets(), self.getItemsCol(),\n",
        "                       frequent_buckets, frequent_pairs, rules)\n",
        "\n",
        "# PCYModel class to store training results\n",
        "class PCYModel(Model, PCYParams, DefaultParamsReadable, DefaultParamsWritable):\n",
        "    def __init__(self, minSupport: Optional[int] = None,\n",
        "                 minConfidence: Optional[float] = None,\n",
        "                 numBuckets: Optional[int] = None,\n",
        "                 itemsCol: Optional[str] = None,\n",
        "                 frequent_buckets: Optional[dict] = None,\n",
        "                 frequent_pairs: Optional[dict] = None,\n",
        "                 rules: Optional[List[Tuple[str, str, float, int]]] = None):\n",
        "\n",
        "        super(PCYModel, self).__init__()\n",
        "\n",
        "        # Set parameters\n",
        "        if minSupport is not None:\n",
        "            self.setMinSupport(minSupport)\n",
        "        if minConfidence is not None:\n",
        "            self.setMinConfidence(minConfidence)\n",
        "        if numBuckets is not None:\n",
        "            self.setNumBuckets(numBuckets)\n",
        "        if itemsCol is not None:\n",
        "            self.setItemsCol(itemsCol)\n",
        "\n",
        "        self.frequent_buckets = frequent_buckets if frequent_buckets is not None else {}\n",
        "        self.frequent_pairs = frequent_pairs if frequent_pairs is not None else {}\n",
        "        self.rules = rules if rules is not None else []\n",
        "\n",
        "        # Get SparkSession from current context\n",
        "        self.spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "    def _transform(self, dataset: DataFrame) -> DataFrame:\n",
        "        # PCY does not transform the data, returning the original dataset\n",
        "        return dataset\n",
        "\n",
        "    def getFrequentBuckets(self) -> dict:\n",
        "        return self.frequent_buckets\n",
        "\n",
        "    def getFrequentPairs(self) -> dict:\n",
        "        return self.frequent_pairs\n",
        "\n",
        "    def getRules(self) -> List[Tuple[str, str, float, int]]:\n",
        "        return self.rules\n",
        "\n",
        "    # Function to print Frequent Pairs as DataFrame\n",
        "    def printFrequentPairs(self):\n",
        "        pairs_data = [(str(pair), count) for pair, count in self.frequent_pairs.items()]\n",
        "        pairs_df = self.spark.createDataFrame(pairs_data, [\"Pair\", \"Support\"])\n",
        "        print(\"Frequent Pairs:\")\n",
        "        pairs_df.show(truncate=False)\n",
        "\n",
        "    # Function to print Association Rules as DataFrame\n",
        "    def printRules(self):\n",
        "        rules_data = [(rule[0], rule[1], rule[2], rule[3]) for rule in self.rules]\n",
        "        rules_df = self.spark.createDataFrame(rules_data, [\"Antecedent\", \"Consequent\", \"Confidence\", \"Support\"])\n",
        "        print(\"Association Rules:\")\n",
        "        rules_df.show(truncate=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize Spark session\n",
        "    spark = SparkSession.builder.appName(\"PCY Algorithm\").getOrCreate()\n",
        "\n",
        "    file_path = \"baskets.csv\"\n",
        "    df = spark.read.csv(file_path, header=True)\n",
        "\n",
        "    # Initialize PCY estimator with parameters directly in constructor\n",
        "    pcy = PCY(minSupport=10, minConfidence=0.2, numBuckets=1000, itemsCol=\"basket\")\n",
        "\n",
        "    model = pcy.fit(df)\n",
        "\n",
        "    model.printFrequentPairs()\n",
        "    model.printRules()\n",
        "\n",
        "    spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
